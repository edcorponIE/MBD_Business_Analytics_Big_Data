series4=data[,4] [1:300]
# 1.	ANALYZING THE SIMULATED TIME SERIES - file Sessions2&3sim.csv
# Reading the files from "my" computer path
data<-read.csv("C:/Users/Eduardo/Desktop/IE 2/Forecasting Time Series/ASSIGMENT 1/Homework_1_DATA.csv",header=TRUE,sep=";",dec=",")
series4=data[,4] [1:300]
#A.	Let's analyze stationarity for each time series
y<-series4# from now, "y" is the data we are going to work with
par(mar=c(1,1,1,1)) # to adjust graphic size
par(mfrow=c(3,1)) # plot the series, its acf and pacf together
ts.plot(y)
acf(y)
pacf(y)
# formal unit root test (Augmented Dickey Fuller test). Testing for stationarity.
# Ho: the process is not stationary. We need, at least, a unit root
# H1: the process is stationary. We have to check different models (lags)
ndiffs(y, alpha=0.05, test=c("adf")) # number of regular differences?
# Before starting, we have to install and activate some R libraries in order to use the necessary statistics and time series functions
# install.packages("fBasics")  # basic statistics
# install.packages("forecast")  # time series functions
library(fBasics)
library(forecast)
series4=data[,4] [1:300]
# formal unit root test (Augmented Dickey Fuller test). Testing for stationarity.
# Ho: the process is not stationary. We need, at least, a unit root
# H1: the process is stationary. We have to check different models (lags)
ndiffs(y, alpha=0.05, test=c("adf")) # number of regular differences?
series5=data[,5] [1:2000]
#A.	Let's analyze stationarity for each time series
y<-series5# from now, "y" is the data we are going to work with
mean(y) # compute basic statistics
#B.	Testing for WHITE NOISE graphically
par(mfrow=c(3,1))
ts.plot(y)
acf(y)
pacf(y)
# Sometimes we will need to do the same for the transformed data "z"
# formal test for white noise (zero autocorrelations)
# Ho: uncorrelated data
# H1: correlated data
Box.test (y, lag = 20, type="Ljung")  # Null: ro1=.=ro20=0
par(mar=c(1,1,1,1)) # to adjust graphic size
par(mfrow=c(3,1)) # plot the series, its acf and pacf together
ts.plot(y)
acf(y)
pacf(y)
#A.	Let's analyze stationarity for each time series
y<-series5# from now, "y" is the data we are going to work with
par(mar=c(1,1,1,1)) # to adjust graphic size
par(mfrow=c(3,1)) # plot the series, its acf and pacf together
ts.plot(y)
acf(y)
ts.plot(y)
acf(y)
pacf(y)
ts.plot(y)
mean(y) # compute basic statistics
# formal unit root test (Augmented Dickey Fuller test). Testing for stationarity.
# Ho: the process is not stationary. We need, at least, a unit root
# H1: the process is stationary. We have to check different models (lags)
ndiffs(y, alpha=0.05, test=c("adf")) # number of regular differences?
# formal normality test
# Ho: the data is normally distributed
# H1: the data is not normally distributed
shapiro.test(y)
series4=data[,4] [1:300]
#A.	Let's analyze stationarity for each time series
y<-series4# from now, "y" is the data we are going to work with
z<-diff(y)
ts.plot(z)
par(mfrow=c(3,1))
ts.plot(z)
acf(z)
pacf(z)
ndiffs(z, alpha=0.05, test=c("adf"))
Box.test (z, lag = 20, type="Ljung")
Box.test (z^2, lag = 20, type="Ljung")
shapiro.test(z)
y <- z
z<-diff(y)
ts.plot(z)
par(mfrow=c(3,1))
ts.plot(z)
acf(z)
pacf(z)
ndiffs(z, alpha=0.05, test=c("adf"))
Box.test (z, lag = 20, type="Ljung")
Box.test (z^2, lag = 20, type="Ljung")
shapiro.test(z)
hist(z,prob=T,ylim=c(0,0.6),xlim=c(mean(z)-3*sd(z),mean(z)+3*sd(z)),col="red")
lines(density(z),lwd=2)
mu<-mean(z)
sigma<-sd(z)
x<-seq(mu-3*sigma,mu+3*sigma,length=100)
yy<-dnorm(x,mu,sigma)
lines(x,yy,lwd=2,col="blue")
datos<-read.csv("C:/Users/Eduardo/Desktop/IE 2/Forecasting Time Series/session 5/Session5data.csv",header=TRUE,sep=";",dec=",")
y<-datos[,2][1:655]      # leave the last 5 observations to compare with the forecasted values
# achieving stationarity and identifying the model
# Is my data stationary? Do I have to take any differences or logs?
par(mar=c(4,4,1,1)) # to adjust graphic size
ts.plot(y)
par(mfrow=c(2,1))
acf(y)
#La data es non stationary porque viendo este plot veo que NO hay un slowly decrease to zero (apunte hecho en session 4)
pacf(y)
ndiffs(y, alpha=0.05, test=c("adf"))
# estimating the model
fit<-arima(y,order=c(0,0,1))
fit # we find the information about the estimated parameters
par(mfrow=c(3,1))
ts.plot(fit$residuals)
acf(fit$residuals)
pacf(fit$residuals)
Box.test(fit$residuals,lag=20)
par(mfrow=c(3,1))
ts.plot(fit$residuals^2)
acf(fit$residuals^2)
pacf(fit$residuals^2)
Box.test(fit$residuals^2,lag=20)
series5=data[,5] [1:2000]
#A.	Let's analyze stationarity for each time series
y<-series5# from now, "y" is the data we are going to work with
par(mar=c(1,1,1,1)) # to adjust graphic size
par(mfrow=c(3,1)) # plot the series, its acf and pacf together
ts.plot(y)
acf(y)
pacf(y)
fit<-arima(y,order=c(2,0,0))    # do we need an AR(2)? Are the residuals WN?
fit # we find the information about the estimated parameters
par(mfrow=c(3,1))
ts.plot(fit$residuals)
acf(fit$residuals)
pacf(fit$residuals)
Box.test(fit$residuals,lag=10)
#Do we need a non-linear model?
par(mfrow=c(3,1))
ts.plot(fit$residuals^2)
acf(fit$residuals^2)
pacf(fit$residuals^2)
Box.test(fit$residuals^2,lag=10)
series7=data[,7] [1:3000]
#A.	Let's analyze stationarity for each time series
y<-series7# from now, "y" is the data we are going to work with
par(mar=c(1,1,1,1)) # to adjust graphic size
par(mfrow=c(3,1)) # plot the series, its acf and pacf together
ts.plot(y)
acf(y)
pacf(y)
mean(y) # compute basic statistics
# formal unit root test (Augmented Dickey Fuller test). Testing for stationarity.
# Ho: the process is not stationary. We need, at least, a unit root
# H1: the process is stationary. We have to check different models (lags)
ndiffs(y, alpha=0.05, test=c("adf")) # number of regular differences?
z<-diff(y)
ts.plot(z)
par(mfrow=c(3,1))
ts.plot(z)
acf(z)
pacf(z)
ndiffs(z, alpha=0.05, test=c("adf"))
Box.test (z, lag = 20, type="Ljung")
Box.test (z^2, lag = 20, type="Ljung")
shapiro.test(z)
shapiro.test(z)
fit<-arima(y,order=c(2,0,0))    # do we need an AR(2)? Are the residuals WN?
fit # we find the information about the estimated parameters
par(mfrow=c(3,1))
ts.plot(fit$residuals)
acf(fit$residuals)
pacf(fit$residuals)
y <- z
z<-diff(y)
y <- z
# 1.	ANALYZING THE SIMULATED TIME SERIES - file Sessions2&3sim.csv
# Reading the files from "my" computer path
data<-read.csv("C:/Users/Eduardo/Desktop/IE 2/Forecasting Time Series/ASSIGMENT 1/Homework_1_DATA.csv",header=TRUE,sep=";",dec=",")
series7=data[,7] [1:3000]
#A.	Let's analyze stationarity for each time series
y<-series7# from now, "y" is the data we are going to work with
z<-diff(y)
y <- z
fit<-arima(y,order=c(2,0,0))    # do we need an AR(2)? Are the residuals WN?
fit # we find the information about the estimated parameters
par(mfrow=c(3,1))
ts.plot(fit$residuals)
acf(fit$residuals)
pacf(fit$residuals)
Box.test(fit$residuals,lag=10)
#Do we need a non-linear model?
par(mfrow=c(3,1))
ts.plot(fit$residuals^2)
acf(fit$residuals^2)
pacf(fit$residuals^2)
Box.test(fit$residuals^2,lag=10)
critics <- read.csv("C:/Users/Eduardo/Desktop/IE 2/Recommendation Engines/session 2/Data/critics.csv")
View(critics)
df_critics <- as.data.frame(critics)
Sophia_critics <- df_critics[df_critics$User=="Sophia",]
Nuria_critics <- df_critics[df_critics$User=="Nuria",]
correlation_Sophia_Nuria <- cor(as.numeric(Sophia_critics[,-1]),as.numeric(Nuria_critics[,-1]),us="complete.obs")
correlation_Sophia_Nuria
df_critics_T <- setNames(data.frame(t(df_critics[,-1])), df_critics[,1])
typeof(df_critics_T)
plotSophiaNuria = ggplot(df_critics_T, aes(x=Sophia, y=Nuria)) +
geom_point() +
geom_smooth(method="lm", se=F) +
labs(x="Sophia Ratings", y="Nuria Ratings", title="Sophia vs. Nuria")
library("ggpubr")
critics <- read.csv("C:/Users/Eduardo/Desktop/IE 2/Recommendation Engines/session 2/Data/critics.csv")
View(critics)
top <- colMeans(critics[,-1], na.rm=TRUE)
top5 <- sort(top, decreasing=TRUE)[1:5]
top5
top5_percentage <- sort(apply(critics[,-1],2,function(x) sum(x>=4, na.rm=T)/sum(x>=0, na.rm=T)),decreasing=T)[1:5]
top5_quantity <- sort(colSums(critics[,-1] > 0, na.rm = TRUE),decreasing=T)[1:5]
top5_quantity
df_critics <- as.data.frame(critics[,-1])
boolean_StarWarsIV <- df_critics[, colnames(df_critics) == "Star.Wars.IV...A.New.Hope"] > 0
df_booleean_StarWarsIV <- df_critics[replace(boolean_StarWarsIV,is.na(boolean_StarWarsIV),FALSE),colnames(df_critics) != "Star.Wars.IV...A.New.Hope"];
number_rows <- nrow(df_booleean_StarWarsIV)
array_top_watched_films = c()
for (i in 1:ncol(df_booleean_StarWarsIV)){
array_top_watched_films[i] <- nrow(na.omit(as.data.frame(df_booleean_StarWarsIV[,i])))/number_rows
##Inside this for we create and array to save all the operations. Here is where I used the commented boolean list to filter and choose only users that have watched Star Wars IV (values TRUE of the list)
}
top_watched_films <- data.frame(colnames(df_critics[,-1]),array_top_watched_films)
solution <- as.data.frame(top_watched_films[order(-array_top_watched_films),])
top5_solution <- solution[1:5,1:2]
top5_solution
View(critics)
boolean_Babe <- df_critics[, colnames(df_critics) == "Babe"] > 0
boolean_critics <- apply(critics[,-1],2,function(x) (x>=4))
boolean_critics[is.na(boolean_critics[,"Babe"])] <- FALSE
Babe_critics <- critics[boolean_critics[,"Babe"],]
Babe_critics_length <- ncol(Babe_critics)-1
Babe_critics <- Babe_critics[,1:Babe_critics_length]
solutionExercise5 <- sort(colMeans(x=as.data.frame(Babe_critics[2:length(Babe_critics)]), na.rm = TRUE), decreasing = T)[1:5]
solutionExercise5
View(df_critics)
library("ggpubr")
critics <- read.csv("C:/Users/Eduardo/Desktop/IE 2/Recommendation Engines/session 2/Data/critics.csv")
df_critics <- as.data.frame(critics)
df_critics_T <- setNames(data.frame(t(df_critics[,-1])), df_critics[,1])
plotSophiaNuria = ggplot(df_critics_T, aes(x=Sophia, y=Nuria)) +
geom_point() +
geom_smooth(method="lm", se=F) +
labs(x="Sophia Ratings", y="Nuria Ratings", title="Sophia vs. Nuria")
plotSophiaNuria
colnames(df_critics_T)
plotMariaNerea = ggplot(df_critics_T, aes(x=Maria, y=Nerea)) +
geom_point() +
geom_text_repel(aes(label=colnames(df_critics_T))) +
geom_smooth(method="lm", se=F) +
labs(x="Maria Ratings", y="Nerea Ratings", title="Maria vs. Nerea")
library("ggrepel")
plotMariaNerea = ggplot(df_critics_T, aes(x=Maria, y=Nerea)) +
geom_point() +
geom_text_repel(aes(label=colnames(df_critics_T))) +
geom_smooth(method="lm", se=F) +
labs(x="Maria Ratings", y="Nerea Ratings", title="Maria vs. Nerea")
plotMariaNerea
rownames(df_critics_T)
plotSophiaNuria = ggplot(df_critics_T, aes(x=Sophia, y=Nuria)) +
geom_point() +
geom_text_repel(aes(label=rownames(df_critics_T))) +
geom_smooth(method="lm", se=F) +
labs(x="Sophia Ratings", y="Nuria Ratings", title="Sophia vs. Nuria")
plotSophiaNuria
plotMariaNerea = ggplot(df_critics_T, aes(x=Maria, y=Nerea)) +
geom_point() +
geom_text_repel(aes(label=rownames(df_critics_T))) +
geom_smooth(method="lm", se=F) +
labs(x="Maria Ratings", y="Nerea Ratings", title="Maria vs. Nerea")
plotChrisJim = ggplot(df_critics_T, aes(x=Chris, y=Jim)) +
geom_point() +
geom_text_repel(aes(label=rownames(df_critics_T))) +
geom_smooth(method="lm", se=F) +
labs(x="Chris Ratings", y="Jim Ratings", title="Chris vs. Jim")
plotChrisJim
df_critics_T
plotSophiaNuria
df_critics_T
plotSophiaNuria
critics <- read.csv("critics.csv")
critics
critics <- read.csv("critics.csv")
